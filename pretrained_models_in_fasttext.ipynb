{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82f256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = openml.datasets.get_dataset(42078)\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "str_cols = [col for col in X.columns if X[col].dtype == \"O\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[str_cols], y, stratify=y, train_size=100_000, test_size=20_000, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddeaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss, precision_score, recall_score\n",
    "\n",
    "def evaluate_clf(clf):\n",
    "  s = time.time()\n",
    "  clf.fit(X_train, y_train)\n",
    "  print(\"Model fit time:\", time.time() - s, \"seconds\")\n",
    "\n",
    "  y_pred = clf.predict(X_test)\n",
    "  y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "  print(\"###### Label Predictions #######\")\n",
    "  print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "  print(\"precision:\", precision_score(y_test, y_pred, average=\"macro\"))\n",
    "  print(\"recall:\", recall_score(y_test, y_pred, average=\"macro\"))\n",
    "  print(\"f1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "  print(\"###### Label Probabilities #######\")\n",
    "  print(\"roc_auc:\", roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovr\"))\n",
    "  print(\"Log loss:\", log_loss(y_test, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60105721",
   "metadata": {},
   "source": [
    "# FastText without pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4493b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread:  557625 lr:  0.000000 avg.loss:  0.203171 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 7.382294178009033 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.9404\n",
      "precision: 0.9218034068597079\n",
      "recall: 0.9117285416291898\n",
      "f1: 0.9151443256724191\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.6337158705198362\n",
      "Log loss: 10.381925401382206\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "import time\n",
    "\n",
    "clf = FastTextClassifier(epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605d0a3",
   "metadata": {},
   "source": [
    "# FastText with pretrained model, dim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87545fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread:  561914 lr: -0.000000 avg.loss:  0.056599 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 109.79233312606812 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  561909 lr:  0.000000 avg.loss:  0.056502 ETA:   0h 0m 0s\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.9439\n",
      "precision: 0.9282315734055383\n",
      "recall: 0.9173211279543442\n",
      "f1: 0.9216053354953253\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.630119287841118\n",
      "Log loss: 10.562863893324028\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss, precision_score, recall_score\n",
    "\n",
    "clf = FastTextClassifier(pretrainedVectors=\"100.vec\", pretrainedDim=100, epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be2987",
   "metadata": {},
   "source": [
    "# FastText with pretrained model, dim=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75971e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress:  99.9% words/sec/thread: 1018124 lr:  0.000230 avg.loss:  0.076783 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 30.50499701499939 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread: 1007620 lr:  0.000000 avg.loss:  0.076694 ETA:   0h 0m 0s\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.9342\n",
      "precision: 0.9136401930033067\n",
      "recall: 0.9039689478178132\n",
      "f1: 0.9072189557118047\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.629000668404463\n",
      "Log loss: 10.562313780726548\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "\n",
    "clf = FastTextClassifier(pretrainedVectors=\"20.vec\", pretrainedDim=20, epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a2a73",
   "metadata": {},
   "source": [
    "# FastText with pretrained model, dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e93c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread: 1460191 lr:  0.000000 avg.loss:  0.087175 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 22.78206992149353 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.91445\n",
      "precision: 0.8916610257116679\n",
      "recall: 0.8727744380166697\n",
      "f1: 0.8800443512569465\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.6017696168896035\n",
      "Log loss: 10.647808005330841\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "\n",
    "clf = FastTextClassifier(pretrainedVectors=\"10.vec\", pretrainedDim=10, epoch=25, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536bdcb7",
   "metadata": {},
   "source": [
    "# Results\n",
    "We have experimented with pretrained models of varying sizes and have found that the largest pretrained model `dim=100` brings slight improvements in the performance in terms of accuracy, recall, and precision, but at a cost of a much higher training time with a magnitude of 15 times. All experiments involving smaller models `dim=[20, 10]` all yield worse results than not using a pretrained model at all.\n",
    "Using a pretrained model therefore barely improves the performance of FastTextClassifier on any of the evaluated metrics, while greatly increasing the training time of the classifier due to the loading time of the pretrained models.\n",
    "\n",
    "Surprisingly, the usage of pretrained models even **lowers** scores that are calculated with class probabilities such as *ROC AUC* and *negative log loss*. We suspect that using pretrained models lowers the confidence of the classification model due to activations of word vectors from the pretrained model. The usage of smaller pretrained models therefore cause signs of underfitting in the downstream classification models.\n",
    "\n",
    "# Conclusion\n",
    "We conclude that the usage of pretrained models **does not improve** the performance of the FastTextClassifier and is in some cases even detrimental to the performance when evaluating the beer review dataset. Not to mention that it also increases the training time significantly due to the loading time of the pretrained models.\n",
    "\n",
    "# Further work\n",
    "We have observed that the use of pretrained models does not bring any improvements on the classifier for this data set. The next step is to experiment whether this hypothesis also holds on other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1475348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce the size of the pretrained model\n",
    "\n",
    "# import fasttext as ft\n",
    "# import fasttext.util\n",
    "\n",
    "# m = ft.load_model(\"cc.en.300.bin\")\n",
    "# m.get_dimension()\n",
    "# fasttext.util.reduce_model(m, 10)\n",
    "# m.save_model(\"cc.en.10.bin\")\n",
    "# m.get_dimension()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82f256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = openml.datasets.get_dataset(42078)\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "str_cols = [col for col in X.columns if X[col].dtype == \"O\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[str_cols], y, stratify=y, train_size=100_000, test_size=20_000, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddeaaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss, precision_score, recall_score\n",
    "\n",
    "def evaluate_clf(clf):\n",
    "  s = time.time()\n",
    "  clf.fit(X_train, y_train)\n",
    "  print(\"Model fit time:\", time.time() - s, \"seconds\")\n",
    "\n",
    "  y_pred = clf.predict(X_test)\n",
    "  y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "  print(\"###### Label Predictions #######\")\n",
    "  print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "  print(\"precision:\", precision_score(y_test, y_pred, average=\"macro\"))\n",
    "  print(\"recall:\", recall_score(y_test, y_pred, average=\"macro\"))\n",
    "  print(\"f1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "  print(\"###### Label Probabilities #######\")\n",
    "  print(\"roc_auc:\", roc_auc_score(y_test, y_probs, average=\"macro\", multi_class=\"ovr\"))\n",
    "  print(\"Log loss:\", log_loss(y_test, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60105721",
   "metadata": {},
   "source": [
    "# FastText without pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4493b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread:  561968 lr:  0.002112 avg.loss:  0.206068 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 7.3589348793029785 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  557209 lr:  0.000000 avg.loss:  0.204000 ETA:   0h 0m 0s\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.94105\n",
      "precision: 0.9231602714157237\n",
      "recall: 0.9123943362459989\n",
      "f1: 0.9163341401263136\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.6338957541492822\n",
      "Log loss: 10.383634787919807\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "import time\n",
    "\n",
    "clf = FastTextClassifier(epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605d0a3",
   "metadata": {},
   "source": [
    "# FastText with pretrained model, dim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87545fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread:  562147 lr:  0.000000 avg.loss:  0.056791 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 109.47481489181519 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.94395\n",
      "precision: 0.9295523875948072\n",
      "recall: 0.9191230299800558\n",
      "f1: 0.9235064620124229\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.6302089306763654\n",
      "Log loss: 10.563789094786433\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss, precision_score, recall_score\n",
    "\n",
    "clf = FastTextClassifier(pretrainedVectors=\"100.vec\", dim=100, epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be2987",
   "metadata": {},
   "source": [
    "# FastText with pretrained model, dim=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75971e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread: 1018112 lr:  0.000000 avg.loss:  0.074384 ETA:   0h 0m 0s 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 30.42029595375061 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.93395\n",
      "precision: 0.9173079778281257\n",
      "recall: 0.9005298518877918\n",
      "f1: 0.9068466232696917\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.6327610671750777\n",
      "Log loss: 10.558693600697756\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "\n",
    "clf = FastTextClassifier(pretrainedVectors=\"20.vec\", dim=20, epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a2a73",
   "metadata": {},
   "source": [
    "# FastText with pretrained model, dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e93c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25698\n",
      "Number of labels: 104\n",
      "Progress: 100.0% words/sec/thread: 1391374 lr:  0.000000 avg.loss:  0.141932 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit time: 19.026647090911865 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Label Predictions #######\n",
      "accuracy: 0.9168\n",
      "precision: 0.8923570240463263\n",
      "recall: 0.8788533308737339\n",
      "f1: 0.8842424539885628\n",
      "###### Label Probabilities #######\n",
      "roc_auc: 0.6134300841898107\n",
      "Log loss: 10.57638846292496\n"
     ]
    }
   ],
   "source": [
    "from gama.configuration.fasttextclassifier import FastTextClassifier\n",
    "\n",
    "clf = FastTextClassifier(pretrainedVectors=\"10.vec\", dim=10, epoch=15, lr=0.2)\n",
    "evaluate_clf(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd3fe7",
   "metadata": {},
   "source": [
    "# Results\n",
    "We have experimented with pretrained models of varying sizes and have found that the largest pretrained model `dim=100` brings slight improvements in the performance in terms of accuracy, recall, and precision, but at a cost of a much higher training time with a magnitude of 15 times. All experiments involving smaller models `dim=[20, 10]` all yield worse results than not using a pretrained model at all.\n",
    "Using a pretrained model therefore barely improves the performance of FastTextClassifier on any of the evaluated metrics, while greatly increasing the training time of the classifier due to the loading time of the pretrained models.\n",
    "\n",
    "Surprisingly, the usage of pretrained models even **lowers** scores that are calculated with class probabilities such as *ROC AUC* and *negative log loss*. We suspect that using pretrained models lowers the confidence of the classification model due to activations of word vectors from the pretrained model. The usage of smaller pretrained models therefore cause underfitting in the downstream classification models.\n",
    "\n",
    "# Conclusion\n",
    "We conclude that the usage of pretrained models **does not improve** the performance of the FastTextClassifier and is in some cases even detrimental to the performance when evaluating the beer review dataset. Not to mention that it also increases the training time significantly due to the loading time of the pretrained models.\n",
    "\n",
    "# Further work\n",
    "We have observed that the use of pretrained models does not bring any improvements on the classifier for this data set. The next step is to experiment whether this hypothesis also holds on other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1475348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce the size of the pretrained model\n",
    "\n",
    "# import fasttext as ft\n",
    "# import fasttext.util\n",
    "\n",
    "# m = ft.load_model(\"cc.en.300.bin\")\n",
    "# m.get_dimension()\n",
    "# fasttext.util.reduce_model(m, 10)\n",
    "# m.save_model(\"cc.en.10.bin\")\n",
    "# m.get_dimension()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
